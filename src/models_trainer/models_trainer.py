# -*- coding: utf-8 -*-
"""Models trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M-GLrr3uNqIpLVCRoaIIcxMBowZLNiBK
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
!nvidia-smi
# %pip install -q transformers

import os
import pandas as pd
from transformers import BertTokenizer
import matplotlib.pyplot as plt
import numpy as np
from transformers import TFBertModel
import datetime

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.metrics import SparseCategoricalAccuracy

import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# Loading data
train = pd.read_csv("07-train.csv", encoding = "utf-8-sig")
valid = pd.read_csv("07-test.csv", encoding = "utf-8-sig")

train["words"] = train["words"].str.replace("¿","")
train["words"] = train["words"].str.replace("?","")
valid["words"] = valid["words"].str.replace("¿","")
valid["words"] = valid["words"].str.replace("?","")

model_name = "bert-base-multilingual-cased"
tokenizer = BertTokenizer.from_pretrained(model_name)

train_sequence_lengths = [len(tokenizer.encode(text))
                          for text in train["words"]]
max_seq_len = max(train_sequence_lengths)
plt.hist(train_sequence_lengths, bins=30)
plt.title(f"max sequence length: {max_seq_len}");

intent_names = train["intent"].unique() 
intent_map = dict((label, idx) for idx, label in enumerate(intent_names))
slot_names = ["[PAD]"]
slot_names += ["B-crop","B-cultivar","I-cultivar","B-locality","I-locality","B-measure","I-measure","B-date","I-date","B-unit","I-unit","O"]
slot_map = {}
for label in slot_names:
    slot_map[label] = len(slot_map)

print(intent_map)
print(slot_map)

def encode_dataset(tokenizer, text_sequences, max_length):
    token_ids = np.zeros(shape=(len(text_sequences), max_length),
                         dtype=np.int32)
    for i, text_sequence in enumerate(text_sequences):
        encoded = tokenizer.encode(text_sequence)
        token_ids[i, 0:len(encoded)] = encoded
    attention_masks = (token_ids != 0).astype(np.int32)
    return {"input_ids": token_ids, "attention_masks": attention_masks}

def encode_token_labels(text_sequences, slot_names, tokenizer, slot_map, max_length):
    encoded = np.zeros(shape=(len(text_sequences), max_length), dtype=np.int32)
    for i, (text_sequence, word_labels) in enumerate(zip(text_sequences, slot_names)):
        encoded_labels = []
        for word, word_label in zip(text_sequence.split(), word_labels.split()):
            tokens = tokenizer.tokenize(word)
            encoded_labels.append(slot_map[word_label])
            expand_label = word_label.replace("B-", "I-")
            if not expand_label in slot_map:
                expand_label = word_label
            encoded_labels.extend([slot_map[expand_label]] * (len(tokens) - 1))
        encoded[i, 1:len(encoded_labels) + 1] = encoded_labels
    return encoded

encoded_train = encode_dataset(tokenizer, train["words"], max_seq_len)
encoded_valid = encode_dataset(tokenizer, valid["words"], max_seq_len)

# print(encoded_train["input_ids"])
# print(encoded_train["attention_masks"])
# print(encoded_valid["input_ids"])
# print(encoded_valid["attention_masks"])

intent_train = train["intent"].map(intent_map).values
intent_valid = valid["intent"].map(intent_map).values

print(intent_train)
print(intent_valid)

slot_train = encode_token_labels(train["words"], train["labels"], tokenizer, slot_map, max_seq_len)
slot_valid = encode_token_labels(valid["words"], valid["labels"], tokenizer, slot_map, max_seq_len)

print(encoded_train["attention_masks"][10])
print(slot_train[10])
#print(slot_valid)

base_bert_model = TFBertModel.from_pretrained(model_name)
base_bert_model.summary()
outputs = base_bert_model(encoded_valid)

len(outputs)
# The first ouput of the BERT model is a tensor with shape: (batch_size, seq_len, output_dim) 
# which computes features for each token in the input sequence:
print(outputs[0].shape)
# The second output of the BERT model is a tensor with shape (batch_size, output_dim) 
# which is the vector representation of the special token [CLS]. 
# This vector is typically used as a pooled representation for the sequence as a whole. 
# This is will be used as the features of our Intent classifier:
print(outputs[1].shape)

class JointIntentAndSlotFillingModel(tf.keras.Model):

    def __init__(self, intent_num_labels=None, slot_num_labels=None, dropout_prob=0.1):        
        super().__init__(name="joint_intent_slot")
        self.bert = TFBertModel.from_pretrained(model_name).bert
        self.dropout = Dropout(dropout_prob)
        self.intent_classifier = Dense(intent_num_labels,
                                       activation='softmax',
                                       name="intent_classifier")
        
        self.slot_classifier = Dense(slot_num_labels,
                                     activation='softmax',
                                     name="slot_classifier")

    def call(self, inputs, **kwargs):   
        
        features = self.bert(inputs, **kwargs)        
        sequence_output = features[0]
        pooled_output = features[1]

        # The first output of the main BERT layer has shape:
        # (batch_size, max_length, output_dim)
        sequence_output = self.dropout(sequence_output, training=kwargs.get("training", False))
        #sequence_output = self.dropout(sequence_output)
        slot_logits = self.slot_classifier(sequence_output)

        # The second output of the main BERT layer has shape:
        # (batch_size, output_dim)
        # and gives a "pooled" representation for the full sequence from the
        # hidden state that corresponds to the "[CLS]" token.
        pooled_output = self.dropout(pooled_output, training=kwargs.get("training", False))
        #pooled_output = self.dropout(pooled_output)
        intent_logits = self.intent_classifier(pooled_output)

        return slot_logits, intent_logits

def create_model(intent_num_labels, slot_num_labels):
  
  model_inputs =  keras.layers.Input(shape=(None,), dtype='int32', name='input_layer')
  
  bert = TFBertModel.from_pretrained(model_name) # get features
  bert_output = bert(model_inputs)    
  
    
  intents_drop = Dropout(rate=0.1, name="intent_dropout")(bert_output[1])  
  intents_fc = Dense(intent_num_labels, activation='softmax', name='intent_classifier')(intents_drop)
  
  slots_output = Dropout(rate=0.3, name='slots_dropout')(bert_output[0])
  slots_output = Dense(500, activation='relu', name='slots_dense')(slots_output)  
  slots_output = Dense(slot_num_labels, activation='softmax', name='slots_classifier')(slots_output)

  model = keras.Model(inputs=model_inputs, outputs=[slots_output, intents_fc])
  return model

#model2 = JointIntentAndSlotFillingModel(intent_num_labels=len(intent_map), slot_num_labels=len(slot_map))
model2 = create_model(intent_num_labels=len(intent_map), slot_num_labels=len(slot_map))

#opt = Adam()#learning_rate=3e-5, epsilon=1e-08)
opt = Adam(learning_rate=3e-5, epsilon=1e-08)
losses = [SparseCategoricalCrossentropy(from_logits=False),
          SparseCategoricalCrossentropy(from_logits=False)]
metrics = [SparseCategoricalAccuracy('accuracy')]
#model.compile(optimizer=opt, loss=losses, metrics=metrics)
model2.compile(optimizer=opt, loss=losses, metrics=metrics)

!rm -dfr log/

log_dir = "log/demeter/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%s")
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)

#history = model.fit(
history = model2.fit(    
    x = [encoded_train["input_ids"],encoded_train["attention_masks"]],
    y = (slot_train, intent_train),
    validation_split=0.2,
    batch_size=90,
    shuffle=True,
    epochs=40,
    callbacks=[tensorboard_callback]
    )

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir log

model2.summary()

tf.saved_model.save(model2, 'demeter_model')

!zip -r demeter_model.zip demeter_model

from google.colab import files
files.download('demeter_model.zip')

#_, train_acc = model.evaluate([encoded_train["input_ids"],encoded_train["attention_masks"]], (slot_train, intent_train))
#_, test_acc = model.evaluate([encoded_valid["input_ids"],encoded_valid["attention_masks"]], (slot_valid, intent_valid))


train_e = model2.evaluate([encoded_train["input_ids"],encoded_train["attention_masks"]], (slot_train, intent_train))
test_e = model2.evaluate([encoded_valid["input_ids"],encoded_valid["attention_masks"]], (slot_valid, intent_valid))

#train_e = model.evaluate([encoded_train["input_ids"],encoded_train["attention_masks"]], (slot_train, intent_train))
#test_e = model.evaluate([encoded_valid["input_ids"],encoded_valid["attention_masks"]], (slot_valid, intent_valid))

#print("train acc", train_acc)
#print("test acc", test_acc)

y_pred = model2.predict([encoded_valid["input_ids"], encoded_valid["attention_masks"]])#.argmax(axis=-1)
#y_pred = model.predict([encoded_valid["input_ids"], encoded_valid["attention_masks"]])#.argmax(axis=-1)

y_intent = y_pred[1].argmax(axis=-1)
cm = confusion_matrix(intent_valid, y_intent)
df_cm = pd.DataFrame(cm, index=intent_map, columns=intent_map)

hmap = sns.heatmap(df_cm, annot=True, fmt="d")
hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
plt.ylabel('True label')
plt.xlabel('Predicted label');

from google.colab import files
#files.download('demeter_m.zip')

def show_predictions(text, tokenizer, my_model, intent_names, slot_names):
    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1
    outputs = my_model(inputs)
    slot_probs, intent_probs = outputs    
    slot_ids = slot_probs.numpy().argmax(axis=-1)[0, 1:-1]    
    intent_id = intent_probs.numpy().argmax(axis=-1)[0]    
    print("## Intent:", intent_names[intent_id])
    print("## Slots:")
    for token, slot_id in zip(tokenizer.tokenize(text), slot_ids):
        print(f"{token:>10} : {slot_names[slot_id]}")

show_predictions("Cúal es el mejor tiempo para sembrar en papa en Ibague", tokenizer, model2, intent_names, slot_names)

def decode_predictions(text, tokenizer, intent_names, slot_names,
                       intent_id, slot_ids):
    info = {"intent": intent_names[intent_id]}
    collected_slots = {}
    active_slot_words = []
    active_slot_name = None
    for word in text.split():
        tokens = tokenizer.tokenize(word)
        current_word_slot_ids = slot_ids[:len(tokens)]
        slot_ids = slot_ids[len(tokens):]
        current_word_slot_name = slot_names[current_word_slot_ids[0]]
        if current_word_slot_name == "O":
            if active_slot_name:
                collected_slots[active_slot_name] = " ".join(active_slot_words)
                active_slot_words = []
                active_slot_name = None
        else:
            # Naive BIO: handling: treat B- and I- the same...
            new_slot_name = current_word_slot_name[2:]
            if active_slot_name is None:
                active_slot_words.append(word)
                active_slot_name = new_slot_name
            elif new_slot_name == active_slot_name:
                active_slot_words.append(word)
            else:
                collected_slots[active_slot_name] = " ".join(active_slot_words)
                active_slot_words = [word]
                active_slot_name = new_slot_name
    if active_slot_name:
        collected_slots[active_slot_name] = " ".join(active_slot_words)
    info["slots"] = collected_slots
    return info

def nlu(text, tokenizer, my_model, intent_names, slot_names):
    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1
    outputs = my_model(inputs)
    slot_logits, intent_logits = outputs
    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, 1:-1]
    intent_id = intent_logits.numpy().argmax(axis=-1)[0]

    return decode_predictions(text, tokenizer, intent_names, slot_names,
                              intent_id, slot_ids)

nlu("Cúal es la climatologia de temperatura máxima en Ibague",tokenizer, model2, intent_names, slot_names)

tf.saved_model.save(model2, 'demeter_model')
from google.colab import files
files.download('demeter_m.zip') 
!zip -r demeter_m.zip demeter_m



model_loaded = tf.saved_model.load("demeter_model")

show_predictions("Cúal es el mejor tiempo para sembrar arroz en Cali", tokenizer, model_loaded, intent_names, slot_names)

!pip show tensorflow

!pip freeze > requirements.txt